"""Click CLI for Phala execution: ``python flow.py phala step ...``

Submits a Metaflow step to Phala Cloud as a CVM workload, then polls for
completion via an S3 sentinel file before cleaning up the CVM.

Execution model (matches @batch / @kubernetes):
  Local:  ``python flow.py phala step <args>``   → this file handles it
  Remote: the CVM runs ``python flow.py step <args>``  (original step command)
"""

from __future__ import annotations

import base64
import glob
import hashlib
import json
import os
import re
import sys
import time
import traceback
from typing import Any

import yaml
from metaflow import util
from metaflow._vendor import click
from metaflow.exception import METAFLOW_EXIT_DISALLOW_RETRY
from metaflow.metadata_provider.util import sync_local_metadata_from_datastore
from metaflow.metaflow_config import DATASTORE_LOCAL_DIR
from metaflow.unbounded_foreach import UBF_CONTROL, UBF_TASK

from .phala_client import PhalaClient, PhalaException

_SENTINEL_PREFIX = ".metaflow-phala/done"
_POLL_INTERVAL = 15  # seconds between sentinel polls

# ---------------------------------------------------------------------------
# Sentinel helper: written to /tmp/mf_sentinel.py inside the CVM.
# Uses boto3 when available; falls back to urllib + presigned PUT URL.
# ---------------------------------------------------------------------------
_SENTINEL_HELPER_PY = """\
import os, sys, urllib.request

bucket  = os.environ.get("METAFLOW_PHALA_SENTINEL_BUCKET", "")
key     = os.environ.get("METAFLOW_PHALA_SENTINEL_KEY", "")
put_url = os.environ.get("METAFLOW_PHALA_SENTINEL_PUT_URL", "")
code    = sys.argv[1] if len(sys.argv) > 1 else "1"

written = False
if bucket and key:
    try:
        import boto3
        boto3.client("s3").put_object(Bucket=bucket, Key=key, Body=code.encode())
        written = True
    except Exception as e:
        print(f"phala sentinel boto3 failed: {e}", file=sys.stderr)

if not written and put_url:
    try:
        req = urllib.request.Request(put_url, data=code.encode(), method="PUT")
        req.add_header("Content-Type", "text/plain")
        urllib.request.urlopen(req, timeout=30)
        written = True
    except Exception as e:
        print(f"phala sentinel presigned failed: {e}", file=sys.stderr)

if not written:
    print("WARNING: phala could not write completion sentinel", file=sys.stderr)
"""


def _sentinel_py_b64() -> str:
    return base64.b64encode(_SENTINEL_HELPER_PY.encode()).decode()


def _make_cvm_name(flow_name: str, step_name: str, task_id: str, retry: int) -> str:
    """Generate a valid Phala CVM name (5-63 chars, starts with letter, a-z0-9-)."""
    uid = hashlib.md5(
        f"{flow_name}/{step_name}/{task_id}/{retry}".encode()
    ).hexdigest()[:10]
    flow_part = re.sub(r"[^a-z0-9]", "-", flow_name.lower())[:12].strip("-")
    step_part = re.sub(r"[^a-z0-9]", "-", step_name.lower())[:10].strip("-")
    name = f"mf-{flow_part}-{step_part}-{uid}"
    return name[:63]


def _parse_s3_sysroot(sysroot: str) -> tuple[str, str]:
    """Parse ``s3://bucket/prefix`` into ``(bucket, prefix)``."""
    path = sysroot.removeprefix("s3://")
    bucket, _, prefix = path.partition("/")
    return bucket, prefix.rstrip("/")


def _build_step_bash_script(setup_cmds: list[str], step_cli: str) -> str:
    """Build the bash script that runs inside the Phala CVM container.

    Structure:
    1. Write /tmp/mf_sentinel.py from METAFLOW_PHALA_SENTINEL_PY env var.
    2. Trap EXIT → sentinel write (runs on any exit, including set -e failures).
    3. set -e: fail fast during environment setup.
    4. Setup commands (pip install, code package download & extract).
    5. set +e: run the actual Metaflow step with exit-code capture.
    6. exit with step exit code (trap fires → sentinel written).
    """
    lines = [
        "#!/bin/bash",
        "# Auto-generated by metaflow-phala.",
        "",
        "# mflog requires MFLOG_STDOUT to be set; send logs to stdout/stderr.",
        "export MFLOG_STDOUT=/dev/stdout",
        "export MFLOG_STDERR=/dev/stderr",
        "",
        "# Decode + save sentinel helper (Python stdlib only — no pip deps needed).",
        "python3 -c \"import os,base64; open('/tmp/mf_sentinel.py','wb')"
        ".write(base64.b64decode(os.environ['METAFLOW_PHALA_SENTINEL_PY']))\"",
        "",
        "# Trap: write completion sentinel on ANY exit (success or failure).",
        "trap 'EC=$?; python3 /tmp/mf_sentinel.py \"$EC\" 2>/dev/null || true; exit $EC' EXIT",
        "",
        "set -e  # Fail fast during setup.",
        "",
    ]
    lines.extend(setup_cmds)
    lines += [
        "",
        "set +e  # Don't abort on step failure.",
        step_cli,
        "STEP_EXIT=$?",
        "exit $STEP_EXIT",
    ]
    return "\n".join(lines)


def _build_compose_yaml(image: str, env: dict[str, str]) -> str:
    """Build the docker-compose YAML string for the Phala CVM.

    Uses yaml.dump with a list-form ``command`` to avoid shell quoting issues.
    The container decodes METAFLOW_PHALA_BASH_SCRIPT and runs it with bash.
    """
    # List-form command: python3 -c "<one-liner>". No shell quoting needed.
    python_one_liner = (
        "import os,sys,subprocess,base64;"
        "b=base64.b64decode(os.environ['METAFLOW_PHALA_BASH_SCRIPT']);"
        "open('/tmp/mf_step.sh','wb').write(b);"
        "sys.exit(subprocess.run(['bash','/tmp/mf_step.sh']).returncode)"
    )

    compose = {
        "services": {
            "metaflow-step": {
                "image": image,
                "restart": "no",
                "environment": env,
                "command": ["python3", "-c", python_one_liner],
            }
        }
    }
    return yaml.dump(compose, default_flow_style=False, allow_unicode=True)


def _poll_sentinel(
    bucket: str,
    key: str,
    timeout: int,
    client: PhalaClient,
    cvm_id: int,
    echo: Any,
) -> int:
    """Poll S3 for the sentinel file.  Returns step exit code."""
    import boto3
    from botocore.exceptions import ClientError

    s3 = boto3.client("s3")
    deadline = time.time() + timeout
    last_log = time.time()

    while time.time() < deadline:
        try:
            obj = s3.get_object(Bucket=bucket, Key=key)
            raw = obj["Body"].read().decode().strip()
            return int(raw)
        except ClientError as e:
            err_code = e.response.get("Error", {}).get("Code", "")
            if err_code not in ("NoSuchKey", "404"):
                echo(f"[phala] S3 poll error ({err_code}): {e}", stream="stderr")

        now = time.time()
        if now - last_log >= 60:
            elapsed = int(now - (deadline - timeout))
            echo(f"[phala] Waiting for step… ({elapsed}s elapsed)", stream="stderr")
            last_log = now

        # Early-exit detection: CVM stopped without writing sentinel
        if client.is_stopped(cvm_id):
            echo(
                "[phala] CVM stopped before sentinel was written — "
                "step likely failed during container setup.",
                stream="stderr",
            )
            return 1

        time.sleep(_POLL_INTERVAL)

    raise TimeoutError(
        f"Phala step did not complete within {timeout}s. "
        f"CVM id={cvm_id}. Check the Phala Cloud dashboard."
    )


def _replay_task_metadata_to_service(
    ctx: Any, run_id: str, step_name: str, task_id: str
) -> None:
    if ctx.obj.metadata.TYPE != "service":
        return
    from metaflow.plugins.metadata_providers.local import LocalMetadataProvider

    meta_dir = LocalMetadataProvider._get_metadir(
        ctx.obj.flow.name, run_id, step_name, task_id
    )
    if not meta_dir:
        return

    metadata_payload, artifact_payload = [], []
    for path in glob.glob(os.path.join(meta_dir, "sysmeta_*.json")):
        with open(path) as f:
            metadata_payload.append(json.load(f))
    for path in glob.glob(os.path.join(meta_dir, "*_artifact_*.json")):
        with open(path) as f:
            artifact_payload.append(json.load(f))

    if not metadata_payload and not artifact_payload:
        return

    provider_cls = ctx.obj.metadata.__class__
    base_url = provider_cls._obj_path(ctx.obj.flow.name, run_id, step_name, task_id)
    ctx.obj.metadata.register_task_id(run_id, step_name, task_id)
    if metadata_payload:
        provider_cls._request(
            ctx.obj.monitor, base_url + "/metadata", "POST", metadata_payload
        )
    if artifact_payload:
        provider_cls._request(
            ctx.obj.monitor, base_url + "/artifact", "POST", artifact_payload
        )


# ---------------------------------------------------------------------------
# Click command group
# ---------------------------------------------------------------------------


@click.group()
def cli() -> None:
    pass


@cli.group(help="Commands related to Phala Cloud execution.")
def phala() -> None:
    pass


@phala.command(
    help="Execute a single Metaflow task inside a Phala Cloud CVM. "
    "Invoked internally by Metaflow — do not call directly."
)
@click.argument("step-name")
@click.argument("code-package-metadata")
@click.argument("code-package-sha")
@click.argument("code-package-url")
@click.option("--image", default=None, help="Docker image.")
@click.option("--cpu", default=2, type=int, help="vCPUs for the CVM.")
@click.option("--memory", default=2048, type=int, help="Memory in MB.")
@click.option("--disk", default=20, type=int, help="Disk size in GB.")
@click.option("--timeout", default=3600, type=int, help="Max seconds to wait.")
@click.option(
    "--env-var",
    "env_vars",
    multiple=True,
    default=None,
    help="Extra env vars in KEY=VALUE format.",
)
@click.option("--run-id", help="Passed to the top-level 'step'.")
@click.option("--task-id", help="Passed to the top-level 'step'.")
@click.option("--input-paths", help="Passed to the top-level 'step'.")
@click.option("--split-index", help="Passed to the top-level 'step'.")
@click.option("--clone-path", help="Passed to the top-level 'step'.")
@click.option("--clone-run-id", help="Passed to the top-level 'step'.")
@click.option("--tag", multiple=True, default=None, help="Passed to the top-level 'step'.")
@click.option("--namespace", default=None, help="Passed to the top-level 'step'.")
@click.option("--retry-count", default=0, help="Passed to the top-level 'step'.")
@click.option("--max-user-code-retries", default=0, help="Passed to the top-level 'step'.")
@click.option(
    "--ubf-context",
    default=None,
    type=click.Choice(["none", UBF_CONTROL, UBF_TASK]),
)
@click.pass_context
def step(
    ctx: Any,
    step_name: str,
    code_package_metadata: str,
    code_package_sha: str,
    code_package_url: str,
    image: str | None,
    cpu: int,
    memory: int,
    disk: int,
    timeout: int,
    env_vars: tuple[str, ...],
    **kwargs: Any,
) -> None:
    def echo(msg: str, stream: str = "stderr", **kw: Any) -> None:
        ctx.obj.echo_always(util.to_unicode(msg), err=(stream == "stderr"), **kw)

    run_id = kwargs["run_id"]
    task_id = kwargs["task_id"]
    retry_count = kwargs.get("retry_count", 0)

    # ------------------------------------------------------------------
    # 1. Build the step CLI (runs inside the container)
    # ------------------------------------------------------------------
    executable = ctx.obj.environment.executable(step_name)
    entrypoint = f"{executable} -u {os.path.basename(sys.argv[0])}"

    top_params = dict(ctx.parent.parent.params)
    if ctx.obj.metadata.TYPE == "service":
        top_params["metadata"] = "local"
    top_args = " ".join(util.dict_to_cli_options(top_params))

    # Split long input_paths into env vars to avoid ARG_MAX limits
    input_paths = kwargs.get("input_paths")
    split_vars: dict[str, str] = {}
    if input_paths:
        chunk = 30 * 1024
        split_vars = {
            f"METAFLOW_INPUT_PATHS_{i // chunk}": input_paths[i : i + chunk]
            for i in range(0, len(input_paths), chunk)
        }
        kwargs["input_paths"] = "".join(f"${{{k}}}" for k in split_vars)

    step_args = " ".join(util.dict_to_cli_options(kwargs))
    step_cli = f"{entrypoint} {top_args} step {step_name} {step_args}"

    # ------------------------------------------------------------------
    # 2. Collect environment variables for the container
    # ------------------------------------------------------------------
    node = ctx.obj.graph[step_name]

    env: dict[str, str] = {
        "METAFLOW_FLOW_FILENAME": os.path.basename(sys.argv[0]),
        "METAFLOW_PHALA_WORKLOAD": "1",
        # Filled in after CVM is created; placeholder here so it's in the compose
        # (the actual CVM id is emitted as task metadata via task_pre_step).
        "METAFLOW_PHALA_CVM_ID": "pending",
    }
    env.update(split_vars)

    # Env vars from @environment decorator on this step
    env_deco = [d for d in node.decorators if d.name == "environment"]
    if env_deco:
        env.update(env_deco[0].attributes["vars"])

    # User env vars from @phala(env={...})
    for item in env_vars:
        k, _, v = item.partition("=")
        if k:
            env[k] = v

    # Datastore + metadata config
    env["METAFLOW_DEFAULT_DATASTORE"] = ctx.obj.flow_datastore.TYPE
    for var in (
        "METAFLOW_DATASTORE_SYSROOT_S3",
        "METAFLOW_DATASTORE_SYSROOT_AZURE",
        "METAFLOW_DATASTORE_SYSROOT_GS",
        "METAFLOW_S3_ENDPOINT_URL",
        "METAFLOW_SERVICE_URL",
        "METAFLOW_DEFAULT_METADATA",
        "METAFLOW_RUNTIME_PROFILE",
        "METAFLOW_PRODUCTION_TOKEN",
    ):
        val = os.environ.get(var)
        if val:
            env[var] = val

    # Cloud credentials
    for var in (
        "AWS_ACCESS_KEY_ID",
        "AWS_SECRET_ACCESS_KEY",
        "AWS_SESSION_TOKEN",
        "AWS_DEFAULT_REGION",
        "AWS_REGION",
        "GOOGLE_APPLICATION_CREDENTIALS",
        "GOOGLE_CLOUD_PROJECT",
        "AZURE_STORAGE_ACCOUNT",
        "AZURE_STORAGE_ACCESS_KEY",
        "AZURE_TENANT_ID",
        "AZURE_CLIENT_ID",
        "AZURE_CLIENT_SECRET",
    ):
        val = os.environ.get(var)
        if val:
            env[var] = val

    # ------------------------------------------------------------------
    # 3. Sentinel: S3 location + presigned PUT URL as fallback
    # ------------------------------------------------------------------
    sysroot = os.environ.get("METAFLOW_DATASTORE_SYSROOT_S3", "")
    sentinel_bucket, sentinel_prefix = _parse_s3_sysroot(sysroot)
    sentinel_key = (
        f"{sentinel_prefix}/{_SENTINEL_PREFIX}"
        f"/{ctx.obj.flow.name}/{run_id}/{step_name}/{task_id}/{retry_count}"
    )

    env["METAFLOW_PHALA_SENTINEL_BUCKET"] = sentinel_bucket
    env["METAFLOW_PHALA_SENTINEL_KEY"] = sentinel_key
    env["METAFLOW_PHALA_SENTINEL_PY"] = _sentinel_py_b64()

    try:
        import boto3

        s3_client = boto3.client("s3")
        put_url = s3_client.generate_presigned_url(
            "put_object",
            Params={"Bucket": sentinel_bucket, "Key": sentinel_key},
            ExpiresIn=max(timeout * 2, 7200),
        )
        env["METAFLOW_PHALA_SENTINEL_PUT_URL"] = put_url
    except Exception as e:
        echo(f"[phala] Could not generate presigned sentinel URL: {e}", stream="stderr")

    # ------------------------------------------------------------------
    # 4. Build the bash step script (base64-encoded as METAFLOW_PHALA_BASH_SCRIPT)
    # ------------------------------------------------------------------
    setup_cmds = ctx.obj.environment.get_package_commands(
        code_package_url,
        ctx.obj.flow_datastore.TYPE,
        code_package_metadata,
    )
    bootstrap_cmds = ctx.obj.environment.bootstrap_commands(
        step_name,
        ctx.obj.flow_datastore.TYPE,
    )

    bash_script = _build_step_bash_script(setup_cmds + bootstrap_cmds, step_cli)
    env["METAFLOW_PHALA_BASH_SCRIPT"] = base64.b64encode(bash_script.encode()).decode()

    if not image:
        image = os.environ.get("METAFLOW_PHALA_IMAGE", "python:3.11-slim")

    # ------------------------------------------------------------------
    # 5. Build docker-compose YAML
    # ------------------------------------------------------------------
    compose_yaml = _build_compose_yaml(image, env)

    # ------------------------------------------------------------------
    # 6. Provision + create CVM
    # ------------------------------------------------------------------
    api_key = ""
    for var in ("PHALA_API_KEY", "METAFLOW_PHALA_API_KEY"):
        api_key = os.environ.get(var, "")
        if api_key:
            break

    client = PhalaClient(api_key)
    cvm_id: int | None = None
    cvm_name = _make_cvm_name(ctx.obj.flow.name, step_name, task_id, retry_count)

    def _sync_metadata() -> None:
        if ctx.obj.metadata.TYPE in ("local", "service"):
            sync_local_metadata_from_datastore(
                DATASTORE_LOCAL_DIR,
                ctx.obj.flow_datastore.get_task_datastore(run_id, step_name, task_id),
            )
            if ctx.obj.metadata.TYPE == "service":
                try:
                    _replay_task_metadata_to_service(ctx, run_id, step_name, task_id)
                except Exception as e:
                    echo(f"[phala] Metadata replay failed: {e}", stream="stderr")

    def _cleanup() -> None:
        if cvm_id is not None:
            try:
                client.delete_cvm(cvm_id)
                echo(f"[phala] Deleted CVM {cvm_id}.", stream="stderr")
            except Exception as e:
                echo(
                    f"[phala] Warning: could not delete CVM {cvm_id}: {e}",
                    stream="stderr",
                )

    compose_file_spec = {
        "name": cvm_name,
        "docker_compose_file": compose_yaml,
        # allowed_envs is for the encrypted-env mechanism; we embed values
        # directly in the compose file for MVP simplicity.
        "allowed_envs": [],
        "kms_enabled": False,
        "public_logs": True,
        "public_sysinfo": False,
    }

    echo(
        f"[phala] Provisioning CVM '{cvm_name}' "
        f"(image={image}, vcpu={cpu}, memory={memory}MB, disk={disk}GB)…",
        stream="stderr",
    )

    try:
        provision_resp = client.provision(
            name=cvm_name,
            compose_file=compose_file_spec,
            vcpu=cpu,
            memory=memory,
            disk_size=disk,
        )
    except Exception as exc:
        echo(f"[phala] Provision failed: {exc}", stream="stderr")
        traceback.print_exc()
        _sync_metadata()
        sys.exit(METAFLOW_EXIT_DISALLOW_RETRY)

    app_id = provision_resp.get("app_id") or provision_resp.get("compose_hash", "")
    compose_hash = provision_resp.get("compose_hash", "")
    echo(
        f"[phala] Provisioned. app_id={app_id[:16]}…, compose_hash={compose_hash[:16]}…",
        stream="stderr",
    )

    try:
        vm = client.create_cvm(app_id=app_id, compose_hash=compose_hash)
    except Exception as exc:
        echo(f"[phala] CVM creation failed: {exc}", stream="stderr")
        traceback.print_exc()
        _sync_metadata()
        sys.exit(METAFLOW_EXIT_DISALLOW_RETRY)

    cvm_id = vm["id"]
    echo(
        f"[phala] CVM {cvm_id} created (status={vm.get('status')}). "
        "Waiting for running state…",
        stream="stderr",
    )

    try:
        client.wait_for_running(cvm_id, timeout=300)
    except PhalaException as exc:
        echo(f"[phala] CVM failed to start: {exc}", stream="stderr")
        _cleanup()
        _sync_metadata()
        sys.exit(METAFLOW_EXIT_DISALLOW_RETRY)

    echo(
        f"[phala] CVM {cvm_id} is running. Polling for completion (timeout={timeout}s)…",
        stream="stderr",
    )

    # ------------------------------------------------------------------
    # 7. Poll for sentinel → get exit code → cleanup
    # ------------------------------------------------------------------
    exit_code = 1
    try:
        exit_code = _poll_sentinel(
            bucket=sentinel_bucket,
            key=sentinel_key,
            timeout=timeout,
            client=client,
            cvm_id=cvm_id,
            echo=echo,
        )
        echo(f"[phala] Step completed with exit code {exit_code}.", stream="stderr")
    except TimeoutError as exc:
        echo(f"[phala] Timeout: {exc}", stream="stderr")
        exit_code = 1
    except Exception as exc:
        echo(f"[phala] Error while polling: {exc}", stream="stderr")
        traceback.print_exc()
        exit_code = 1
    finally:
        _cleanup()
        _sync_metadata()

    if exit_code != 0:
        sys.exit(exit_code)
